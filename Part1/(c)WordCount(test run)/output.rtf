{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs28 \cf2 \CocoaLigature0 sh-4.1# bin/hadoop jar wordcount.jar WordCount /user/cloudera/wordcount/input /user/cloudera/wordcount/output\
20/06/03 20:26:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\
20/06/03 20:26:53 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\
20/06/03 20:26:54 INFO input.FileInputFormat: Total input paths to process : 3\
20/06/03 20:26:54 INFO mapreduce.JobSubmitter: number of splits:3\
20/06/03 20:26:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1591217769083_0002\
20/06/03 20:26:54 INFO impl.YarnClientImpl: Submitted application application_1591217769083_0002\
20/06/03 20:26:54 INFO mapreduce.Job: The url to track the job: http://35353340e4ce:8088/proxy/application_1591217769083_0002/\
20/06/03 20:26:54 INFO mapreduce.Job: Running job: job_1591217769083_0002\
20/06/03 20:27:00 INFO mapreduce.Job: Job job_1591217769083_0002 running in uber mode : false\
20/06/03 20:27:00 INFO mapreduce.Job:  map 0% reduce 0%\
20/06/03 20:27:06 INFO mapreduce.Job:  map 100% reduce 0%\
20/06/03 20:27:13 INFO mapreduce.Job:  map 100% reduce 100%\
20/06/03 20:27:13 INFO mapreduce.Job: Job job_1591217769083_0002 completed successfully\
20/06/03 20:27:13 INFO mapreduce.Job: Counters: 49\
	File System Counters\
		FILE: Number of bytes read=200\
		FILE: Number of bytes written=461017\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
		HDFS: Number of bytes read=461\
		HDFS: Number of bytes written=80\
		HDFS: Number of read operations=12\
		HDFS: Number of large read operations=0\
		HDFS: Number of write operations=2\
	Job Counters \
		Launched map tasks=3\
		Launched reduce tasks=1\
		Data-local map tasks=3\
		Total time spent by all maps in occupied slots (ms)=12138\
		Total time spent by all reduces in occupied slots (ms)=4257\
		Total time spent by all map tasks (ms)=12138\
		Total time spent by all reduce tasks (ms)=4257\
		Total vcore-seconds taken by all map tasks=12138\
		Total vcore-seconds taken by all reduce tasks=4257\
		Total megabyte-seconds taken by all map tasks=12429312\
		Total megabyte-seconds taken by all reduce tasks=4359168\
	Map-Reduce Framework\
		Map input records=3\
		Map output records=18\
		Map output bytes=158\
		Map output materialized bytes=212\
		Input split bytes=375\
		Combine input records=0\
		Combine output records=0\
		Reduce input groups=12\
		Reduce shuffle bytes=212\
		Reduce input records=18\
		Reduce output records=12\
		Spilled Records=36\
		Shuffled Maps =3\
		Failed Shuffles=0\
		Merged Map outputs=3\
		GC time elapsed (ms)=86\
		CPU time spent (ms)=2320\
		Physical memory (bytes) snapshot=882470912\
		Virtual memory (bytes) snapshot=2958602240\
		Total committed heap usage (bytes)=655360000\
	Shuffle Errors\
		BAD_ID=0\
		CONNECTION=0\
		IO_ERROR=0\
		WRONG_LENGTH=0\
		WRONG_MAP=0\
		WRONG_REDUCE=0\
	File Input Format Counters \
		Bytes Read=86\
	File Output Format Counters \
		Bytes Written=80\
sh-4.1# bin/hadoop fs -cat /user/cloudera/wordcount/output/*\
Hadoop	3\
Oh	1\
a	1\
an	1\
as	2\
be	1\
can	1\
elephant	1\
fellow	1\
is	3\
what	1\
yellow	2}