{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs28 \cf2 \CocoaLigature0 sh-4.1# bin/hadoop jar InMapperWordCount.jar InMapperWordCount /user/cloudera/wordcount/input /user/cloudera/wordcount/output\
20/06/03 21:24:53 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\
20/06/03 21:24:54 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\
20/06/03 21:24:54 INFO input.FileInputFormat: Total input paths to process : 3\
20/06/03 21:24:54 INFO mapreduce.JobSubmitter: number of splits:3\
20/06/03 21:24:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1591233743118_0001\
20/06/03 21:24:55 INFO impl.YarnClientImpl: Submitted application application_1591233743118_0001\
20/06/03 21:24:55 INFO mapreduce.Job: The url to track the job: http://35353340e4ce:8088/proxy/application_1591233743118_0001/\
20/06/03 21:24:55 INFO mapreduce.Job: Running job: job_1591233743118_0001\
20/06/03 21:25:02 INFO mapreduce.Job: Job job_1591233743118_0001 running in uber mode : false\
20/06/03 21:25:02 INFO mapreduce.Job:  map 0% reduce 0%\
20/06/03 21:25:08 INFO mapreduce.Job:  map 100% reduce 0%\
20/06/03 21:25:14 INFO mapreduce.Job:  map 100% reduce 100%\
20/06/03 21:25:14 INFO mapreduce.Job: Job job_1591233743118_0001 completed successfully\
20/06/03 21:25:14 INFO mapreduce.Job: Counters: 49\
	File System Counters\
		FILE: Number of bytes read=191\
		FILE: Number of bytes written=461063\
		FILE: Number of read operations=0\
		FILE: Number of large read operations=0\
		FILE: Number of write operations=0\
		HDFS: Number of bytes read=461\
		HDFS: Number of bytes written=80\
		HDFS: Number of read operations=12\
		HDFS: Number of large read operations=0\
		HDFS: Number of write operations=2\
	Job Counters \
		Launched map tasks=3\
		Launched reduce tasks=1\
		Data-local map tasks=3\
		Total time spent by all maps in occupied slots (ms)=13436\
		Total time spent by all reduces in occupied slots (ms)=3120\
		Total time spent by all map tasks (ms)=13436\
		Total time spent by all reduce tasks (ms)=3120\
		Total vcore-seconds taken by all map tasks=13436\
		Total vcore-seconds taken by all reduce tasks=3120\
		Total megabyte-seconds taken by all map tasks=13758464\
		Total megabyte-seconds taken by all reduce tasks=3194880\
	Map-Reduce Framework\
		Map input records=3\
		Map output records=17\
		Map output bytes=151\
		Map output materialized bytes=203\
		Input split bytes=375\
		Combine input records=0\
		Combine output records=0\
		Reduce input groups=12\
		Reduce shuffle bytes=203\
		Reduce input records=17\
		Reduce output records=12\
		Spilled Records=34\
		Shuffled Maps =3\
		Failed Shuffles=0\
		Merged Map outputs=3\
		GC time elapsed (ms)=140\
		CPU time spent (ms)=2180\
		Physical memory (bytes) snapshot=904871936\
		Virtual memory (bytes) snapshot=2938429440\
		Total committed heap usage (bytes)=656408576\
	Shuffle Errors\
		BAD_ID=0\
		CONNECTION=0\
		IO_ERROR=0\
		WRONG_LENGTH=0\
		WRONG_MAP=0\
		WRONG_REDUCE=0\
	File Input Format Counters \
		Bytes Read=86\
	File Output Format Counters \
		Bytes Written=80\
sh-4.1# bin/hadoop fs -cat /user/cloudera/wordcount/output/*\
Hadoop	3\
Oh	1\
a	1\
an	1\
as	2\
be	1\
can	1\
elephant	1\
fellow	1\
is	3\
what	1\
yellow	2\
sh-4.1# \
}